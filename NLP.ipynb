{"cells":[{"cell_type":"markdown","metadata":{"id":"JXAvwEn3DANx"},"source":["# Natural Language Processing\n","## Dependencies\n","First,  install the following dependencies:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiWw3lIHDAN0"},"outputs":[],"source":["!pip install datasets scikit-learn huggingface_hub transformers==4.37.2 evaluate accelerate"]},{"cell_type":"markdown","metadata":{"id":"LFr5xDtZDAN1"},"source":["## Dataset\n","We are going to use the IMDB dataset for sentiment classification"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"IPCtQBfIDAN1","outputId":"f83badba-0e11-4068-b17a-f472667eedb9"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"imdb\")\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"DvJktUxYDAN2"},"source":["## Subset generation\n","For the sake of this example, we are going to use a subset of the dataset."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sgnbARanDAN2","outputId":"d913c3c6-59b6-40a5-db50-c5325ad11496"},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 5000\n"," }),\n"," label\n"," 0    2500\n"," 1    2500\n"," Name: count, dtype: int64)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dataset_subset = dataset['train'].train_test_split(test_size=0.2, stratify_by_column='label', shuffle=True, seed=42)['test']\n","dataset_subset, dataset_subset.to_pandas()['label'].value_counts(),"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4bU7jcKIDAN3","outputId":"adc27000-0a0d-4c7b-df1e-37fd1d7913be"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 5000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'] = dataset_subset\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"27tk9fwYDAN3"},"source":["## The Naive Bayes Classifier in a Nutshell\n","We treat the possibility of a word appearing in a document as independent from the possibility of another word appearing in the same document. This is a naive assumption, but it allows us to simplify the calculation of the probability of a document belonging to a class.\n","\n","### Mathematical Formulation\n","\n","Given a document $D$ with words $w_1, w_2, ..., w_n$ and a set of classes $C$, the Naive Bayes classifier computes the posterior probability of each class $c \\in C$ given the document $D$, $P(c | D)$, as follows:\n","\n","$$P(c | D) = \\frac{P(c) \\prod_{i=1}^{n} P(w_i | c)}{P(D)}$$\n","\n","where:\n","- $P(c | D)$ is the posterior probability of class $c$ given document $D$,\n","- $P(c)$ is the prior probability of class $c$,\n","- $P(w_i | c)$ is the likelihood of word $w_i$ given class $c$,\n","- $P(D)$ is the probability of document $D$, which acts as a normalizing constant.\n","\n","Since $P(D)$ is the same for all classes, we focus on maximizing $P(c) \\prod_{i=1}^{n} P(w_i | c)$.\n","\n","### Handling Out-of-Vocabulary Words with Laplace Smoothing\n","\n","A challenge in applying Naive Bayes to text classification is dealing with Out-of-Vocabulary (OOV) words—words present in the test set but not seen during training. To address this, we use Laplace smoothing, which adjusts the likelihood calculation to ensure no zero probabilities.\n","\n","#### Laplace Smoothing Formula\n","\n","The modified formula for $P(w_i | c)$ with Laplace smoothing is:\n","\n","$$P(w_i | c) = \\frac{N_{w_i, c} + \\alpha}{N_c + \\alpha \\times |V|}$$\n","\n","where:\n","- $N_{w_i, c}$ is the count of times word $w_i$ appears in documents of class $c$,\n","- $N_c$ is the total count of all words in documents of class $c$,\n","- $|V|$ is the size of the vocabulary,\n","- $\\alpha$ is the smoothing parameter (typically set to 1 for add-one smoothing).\n","\n","This adjustment ensures that every word, including those not seen during training, contributes to the probability calculations, allowing the classifier to make predictions even in the presence of new words."]},{"cell_type":"markdown","metadata":{"id":"R6pi7ZMKDAN4"},"source":["## `TODO`: Implementing NB Classifier from Scratch\n","Generally,  your algorithm should produce a model with acc around 0.7. The following two cells are the main part of the implementation."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dT-5HzwiDAN4"},"outputs":[],"source":["from collections import Counter, defaultdict\n","def train_model(sentences, labels):\n","    '''\n","    sentences: list of strings\n","    labels: list of 0 or 1s\n","\n","    returns: (wscores, cscores)\n","    - wscores: a dictionary of dicts, where wscores[label][word] is the probability of word given label\n","    - cscores: a dict, where cscores[label] is the probability of label\n","    '''\n","    wscores = defaultdict(Counter)\n","    cscores = Counter(labels)\n","    for sentence, label in zip(sentences, labels):\n","        # add up the word counts for the sentence\n","        wscores[label] += Counter(sentence.split())\n","    # since the keys of wscores[label] are the words in the sentence, we can get the total vocab size by taking the union of all the keys\n","    total_vocab_size = len(set.union(*(set(wscores[l].keys()) for l in wscores)))\n","    to_return_wscores = {}\n","    for label in cscores:\n","        # normalize label counts\n","        cscores[label] /= len(labels)\n","        # total word count for this label\n","        label_word_count = sum(wscores[label].values())\n","        for word in wscores[label]:\n","            wscores[label][word] = (wscores[label][word] + 1) / (label_word_count + total_vocab_size)\n","        # handle oov for this label\n","        to_return_wscores[label] = defaultdict(lambda: 1/(label_word_count + total_vocab_size))\n","        to_return_wscores[label].update(dict(wscores[label]))\n","    return to_return_wscores, dict(cscores)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"k9rpWMTSDAN5"},"outputs":[],"source":["import math\n","def inference_model_ele(wscores, cscores, sentence):\n","    '''\n","    Helper function, may vary.\n","    '''\n","    scores = Counter()\n","    for label in cscores:\n","        scores[label] = math.log(cscores[label])\n","        for word in sentence.split():\n","            scores[label] += math.log(wscores[label][word] + 1e-10)\n","\n","    return scores.most_common(1)[0][0]\n","\n","def inference_model(wscores, cscores, sentences):\n","    '''\n","    returns: list of 0 or 1s using our model (wscores, cscores)\n","    '''\n","    return [inference_model_ele(wscores, cscores, sentence) for sentence in sentences]"]},{"cell_type":"markdown","metadata":{"id":"RQIYbloIDAN5"},"source":["## Putting Everything Together"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8nERo-roDAN5","outputId":"cf223de2-56e2-4637-b095-b6b806fe2ab6"},"outputs":[{"data":{"text/plain":["0.80736"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","wscores, cscores = train_model(dataset['train']['text'], dataset['train']['label'])\n","predictions = inference_model(wscores, cscores, dataset['test']['text'])\n","accuracy = accuracy_score(dataset['test']['label'], predictions)\n","accuracy"]},{"cell_type":"markdown","metadata":{"id":"IM3skZOxDAN6"},"source":["## Using the Naive Bayes Classifier from `sklearn`\n","We see a surge in performance because there are many optimizations in the `sklearn` implementation that we did not consider.\n","\n","NB is also a strong baseline on small datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK6Ae96BDAN6"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(stop_words='english', )\n","# vectorizer = CountVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n","X_train = vectorizer.fit_transform(dataset['train']['text'])\n","X_test = vectorizer.transform(dataset['test']['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8s45fLwZDAN6","outputId":"3c151b93-3c80-4d97-85f1-31f3ea30d757"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"],"text/plain":["MultinomialNB()"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","y_train = dataset['train']['label']\n","y_test = dataset['test']['label']\n","\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXEuOlLkDAN7","outputId":"b12f53f3-3b07-4bf9-b0f2-8ccdb4f090ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.81164\n"]}],"source":["from sklearn.metrics import accuracy_score\n","\n","predictions = classifier.predict(X_test)\n","accuracy = accuracy_score(y_test, predictions)\n","print(f\"Accuracy: {accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"NDXgz5UDDAN7"},"source":["## Using Pretrained Embeddings (GloVe)\n","In our previous example, there is not much information encoded about the words. They are encoded as independent co-ocurrence frequencies. On the other hand,  pretrained embeddings are trained on a large corpus to predict the context of a word. This means that the embeddings contains more information about the words.\n","\n","The following illustration is taken from [CS224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/slides/cs224n-2023-lecture02-wordvecs2.pdf). It is used to illustrate the training of word2vec, but it also illustrates the general idea of training word embeddings.\n","\n","![word2vec](https://drive.google.com/uc?export=view&id=1rrACOt5WeoVbynB-pcC6h_O-ys1wySgU)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1YjMkHRVDAN7"},"source":["## Loading Glove"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga0KyNnFDAN7"},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","hf_hub_download(repo_id=\"JeremiahZ/glove\", filename=\"glove.6B.50d.txt\", local_dir=\".\", )\n","hf_hub_download(repo_id=\"JeremiahZ/glove\", filename=\"run_classification.py\", local_dir=\".\", )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6LKMsjMDAN7","outputId":"a8a5675b-3446-4b61-c6ce-a9644b69e1ff"},"outputs":[{"data":{"text/plain":["(50,)"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from collections import defaultdict\n","def load_glove_embeddings(path):\n","    embeddings_dict = {}\n","    with open(path, 'r', encoding='utf8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            vector = np.asarray(values[1:], \"float32\")\n","            embeddings_dict[word] = vector\n","    oov_embedding = np.mean(list(embeddings_dict.values()), axis=0)\n","    to_return_dict = defaultdict(lambda: oov_embedding)\n","    to_return_dict.update(embeddings_dict)\n","    return to_return_dict\n","\n","glove_path = './glove.6B.50d.txt'  # Update this path\n","glove_embeddings = load_glove_embeddings(glove_path)\n","glove_embeddings['the'].shape"]},{"cell_type":"markdown","metadata":{"id":"lT0J91jzDAN8"},"source":["## Logistic Regression with GloVe\n","We are going to start with a baseline model and improve upon it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ww_3lDY_DAN8"},"outputs":[],"source":["def text_to_embedding(text, embeddings_dict):\n","    words = text.split()\n","    embeddings = [embeddings_dict[word] for word in words]\n","    if embeddings:\n","        return np.mean(embeddings, axis=0)\n","    else:\n","        return np.zeros(50)  # We should always get a embedding since we are using defaultdict, this is out of caution\n","\n","X_train_embeddings = np.array([text_to_embedding(text, glove_embeddings) for text in dataset['train']['text']])\n","X_test_embeddings = np.array([text_to_embedding(text, glove_embeddings) for text in dataset['test']['text']])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tyeFBZpDAN8","outputId":"cf126e26-fced-4a0b-967a-ebf9e2987662"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"],"text/plain":["LogisticRegression(max_iter=1000)"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.linear_model import LogisticRegression\n","\n","y_train = dataset['train']['label']\n","y_test = dataset['test']['label']\n","\n","logistic_model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n","logistic_model.fit(X_train_embeddings, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_b5pjPpDAN8","outputId":"5bbc5e2c-624a-4c69-fc55-c4e44cc4addb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7336\n"]}],"source":["predictions = logistic_model.predict(X_test_embeddings)\n","accuracy = accuracy_score(y_test, predictions)\n","print(f\"Accuracy: {accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"e4B2M-lYDAN9"},"source":["## GRU with GloVe\n","First,  we need to preprocess our dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SPXeVxODAN9"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","# Tokenization\n","tokenizer = Tokenizer(num_words=10000)  # Limiting our vocabulary to the top 10,000 words\n","tokenizer.fit_on_texts(dataset['train']['text'])\n","\n","# Convert texts to sequences of indices\n","X_train_seq = tokenizer.texts_to_sequences(dataset['train']['text'])\n","X_test_seq = tokenizer.texts_to_sequences(dataset['test']['text'])\n","\n","# Pad sequences to ensure uniform length\n","maxlen = 100  # This can be adjusted\n","X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n","X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n","\n","# Convert labels to tensors\n","y_train = torch.tensor(dataset['train']['label'])\n","y_test = torch.tensor(dataset['test']['label'])\n","\n","# Dataset\n","class TextDataset(Dataset):\n","    def __init__(self, sequences, labels):\n","        self.sequences = sequences\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.sequences[idx], dtype=torch.long), self.labels[idx]\n","\n","train_dataset = TextDataset(X_train_pad, y_train)\n","test_dataset = TextDataset(X_test_pad, y_test)\n","\n","# DataLoader\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"x6IBQZ65DAN9"},"source":["## `TODO`: Defining the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tghdGPBJDAN9"},"outputs":[],"source":["import torch.nn as nn\n","# Model instantiation\n","vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n","embedding_dim = 50  # GloVe 50d\n","hidden_dim = 64\n","output_dim = 1\n","\n","class GRUModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embeddings):\n","        '''\n","        1. An embedding layer\n","        2. A GRU layer\n","        3. A linear layer\n","\n","        We are initing the embedding layer with the GloVe embeddings\n","        '''\n","        super(GRUModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(embeddings)  # Initialize embedding layer with GloVe embeddings\n","        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        \n","    def forward(self, x):\n","        x = self.embedding(x)\n","        _, hidden = self.gru(x)\n","        out = self.fc(hidden.squeeze(0))\n","        return out\n","\n","# TODO: Prepare the GloVe embedding matrix of size (vocab_size, hidden_dim)\n","embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n","for word, i in tokenizer.word_index.items():\n","    if i < vocab_size:\n","        embedding_vector = glove_embeddings.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = torch.tensor(embedding_vector, dtype=torch.float32)\n","\n","\n","model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix)"]},{"cell_type":"markdown","metadata":{"id":"fXF6KNKHDAN-"},"source":["## Train!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eol-RyPDAN-","outputId":"6fdcb688-16b4-463a-bc80-c2da08944993"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Step 10, Loss: 0.6919932782649993\n","Epoch 1, Step 20, Loss: 0.6947023630142212\n","Epoch 1, Step 30, Loss: 0.6948124051094056\n","Epoch 1, Step 40, Loss: 0.6845528841018677\n","Epoch 1, Step 50, Loss: 0.6874830543994903\n","Epoch 1, Step 60, Loss: 0.6788933992385864\n","Epoch 1, Step 70, Loss: 0.6717429339885712\n","Epoch 1, Step 80, Loss: 0.6770426809787751\n","Epoch 1, Step 90, Loss: 0.6561667263507843\n","Epoch 1, Step 100, Loss: 0.6259108662605286\n","Epoch 1, Step 110, Loss: 0.6040982604026794\n","Epoch 1, Step 120, Loss: 0.5965084969997406\n","Epoch 1, Step 130, Loss: 0.6147490441799164\n","Epoch 1, Step 140, Loss: 0.7254113852977753\n","Epoch 1, Step 150, Loss: 0.6236443042755127\n","Epoch 2, Step 10, Loss: 0.6353871285915375\n","Epoch 2, Step 20, Loss: 0.6063235342502594\n","Epoch 2, Step 30, Loss: 0.5640124976634979\n","Epoch 2, Step 40, Loss: 0.5351787120103836\n","Epoch 2, Step 50, Loss: 0.5364161550998687\n","Epoch 2, Step 60, Loss: 0.5556027203798294\n","Epoch 2, Step 70, Loss: 0.485073384642601\n","Epoch 2, Step 80, Loss: 0.5257476329803467\n","Epoch 2, Step 90, Loss: 0.49903541803359985\n","Epoch 2, Step 100, Loss: 0.4861141204833984\n","Epoch 2, Step 110, Loss: 0.4449624687433243\n","Epoch 2, Step 120, Loss: 0.5167663782835007\n","Epoch 2, Step 130, Loss: 0.43045546412467955\n","Epoch 2, Step 140, Loss: 0.4434887647628784\n","Epoch 2, Step 150, Loss: 0.41764249503612516\n","Epoch 3, Step 10, Loss: 0.3243947088718414\n","Epoch 3, Step 20, Loss: 0.3586738258600235\n","Epoch 3, Step 30, Loss: 0.3723700940608978\n","Epoch 3, Step 40, Loss: 0.3872027456760406\n","Epoch 3, Step 50, Loss: 0.38449664413928986\n","Epoch 3, Step 60, Loss: 0.33016142547130584\n","Epoch 3, Step 70, Loss: 0.3725327908992767\n","Epoch 3, Step 80, Loss: 0.3308072179555893\n","Epoch 3, Step 90, Loss: 0.30990673750638964\n","Epoch 3, Step 100, Loss: 0.40598426163196566\n","Epoch 3, Step 110, Loss: 0.3566901445388794\n","Epoch 3, Step 120, Loss: 0.3919597238302231\n","Epoch 3, Step 130, Loss: 0.35047118067741395\n","Epoch 3, Step 140, Loss: 0.314944851398468\n","Epoch 3, Step 150, Loss: 0.32738643884658813\n"]}],"source":["# device = torch.device()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","model = model.to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","epochs = 3\n","log_steps = 10\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    step_count = 0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs).squeeze(1)\n","        loss = criterion(outputs, labels.float())\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        step_count += 1\n","        if step_count % log_steps == 0:\n","            print(f'Epoch {epoch+1}, Step {step_count}, Loss: {total_loss/log_steps}')\n","            total_loss = 0\n","\n","    # print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n"]},{"cell_type":"markdown","metadata":{"id":"ifjxpoqgDAN-"},"source":["## Evaluate Our Model on the Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onziDW9sDAN_","outputId":"5e8493b7-7a70-459c-dded-64d308c1147f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8214\n"]}],"source":["model.eval()\n","total = 0\n","correct = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs).squeeze(1)\n","        predicted = torch.round(torch.sigmoid(outputs))\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Test Accuracy: {correct / total}')"]},{"cell_type":"markdown","metadata":{"id":"bMQBycpdDAN_"},"source":["## `TODO`: Imdb Review Classification with distilBERT\n","We are going to reuse the pipeline that Huggingface provides in the official `transformers` library. Please go through the arguments defined in `run_classification.py` and Huggingface [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) to write a script that trains a distilBERT model on the IMDB dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QohrV1z9DAN_"},"outputs":[],"source":["!python run_classification.py \\\n","    --model_name_or_path distilbert-base-uncased \\\n","    --dataset_name imdb \\\n","    --remove_splits unsupervised \\\n","    --shuffle_train_dataset \\\n","    --text_column_names text \\\n","    --metric_name accuracy \\\n","    --do_eval \\\n","    --validation_split_name test \\\n","    --max_seq_length 128 \\\n","    --per_device_train_batch_size 32 \\\n","    --per_device_eval_batch_size 64 \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --output_dir ./checkpoints/${dataset} \\\n","    --save_strategy no \\\n","    --do_train"]},{"cell_type":"markdown","metadata":{"id":"uiHiPmFeDAN_"},"source":["## Reference Eval Metrics\n","```bash\n","***** eval metrics *****\n","  eval_accuracy           =       0.87\n","  eval_loss               =     0.3005\n","  eval_runtime            = 0:03:10.01\n","  eval_samples            =      25000\n","  eval_samples_per_second =    131.568\n","  eval_steps_per_second   =      2.058\n","```"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"hgf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
