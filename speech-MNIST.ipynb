{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech MNIST\n",
    "We are operating on melspectrogram features. Due to the similarity with images, we can use a CNN to classify the spoken digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jayrodge/AudioMNIST-using-PyTorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "os.chdir('AudioMNIST-using-PyTorch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = (image / 255.0) - 0.5\n",
    "    return image\n",
    "\n",
    "def get_filenames_and_labels(root_dir):\n",
    "    '''\n",
    "    Returns a list of filenames and a list of labels.\n",
    "    - Subfolders under `AudioMNIST-using-PyTorch/MNIST/` corresponds to the labels.\n",
    "        - Eg. `AudioMNIST-using-PyTorch/MNIST/00/` contains all the files with label 0.\n",
    "    - Labels should be integers from 0 to 9.\n",
    "    '''\n",
    "    # TODO: Fill me\n",
    "    pass\n",
    "\n",
    "def split_data(filenames, labels, test_size=0.2, valid_size=0.2):\n",
    "    # Split into train and temp (test + valid)\n",
    "    filenames_train, filenames_temp, labels_train, labels_temp = train_test_split(\n",
    "        filenames, labels, test_size=(test_size + valid_size), stratify=labels, random_state=42)\n",
    "    \n",
    "    # TODO: Split temp into test and valid\n",
    "    \n",
    "    return filenames_train, labels_train, filenames_test, labels_test, filenames_valid, labels_valid\n",
    "\n",
    "def create_dataset(filenames, labels, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(lambda x, y: (load_and_preprocess_image(x), tf.cast(y, tf.int32)),\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=len(filenames))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'MNIST'\n",
    "batch_size = 128\n",
    "\n",
    "# Get filenames and labels\n",
    "filenames, labels = get_filenames_and_labels(root_dir)\n",
    "\n",
    "# TODO: Split data (One line of code)\n",
    "\n",
    "# TODO: Create datasets (Three lines of code)\n",
    "\n",
    "# Logging the dataset information along with the number of samples\n",
    "print(\"Train Dataset:\", train_dataset, \"Number of Samples:\", len(filenames_train))\n",
    "print(\"Test Dataset:\", test_dataset, \"Number of Samples:\", len(filenames_test))\n",
    "print(\"Validation Dataset:\", valid_dataset, \"Number of Samples:\", len(filenames_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "classes = [str(i) for i in range(10)]  # If your classes are labeled 0 through 9\n",
    "# Function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img * 0.5 + 0.5  # unnormalize\n",
    "    plt.imshow(img)  # No need to transpose\n",
    "\n",
    "# Obtain one batch of training images\n",
    "for images, labels in train_dataset.take(1):\n",
    "    images = images.numpy()  # Convert images to numpy for display\n",
    "    labels = labels.numpy()  # Convert labels to numpy for display\n",
    "\n",
    "# Plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# Display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(models.Model):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # TODO: fill me\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # TODO: fill me\n",
    "        return x\n",
    "\n",
    "# Create the model instance\n",
    "model = Net()\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.build((None, 224, 224, 3))  # `None` can accommodate a variable batch size\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_function = SparseCategoricalCrossentropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_function,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 2  # You may increase this number, but 2 epochs works well enough\n",
    "\n",
    "# Callback for saving the best model in the TensorFlow SavedModel format\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('model_MNIST', save_best_only=True, save_format=\"tf\")\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=valid_dataset,\n",
    "                    epochs=n_epochs,\n",
    "                    callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Testing the CNN Model\n",
    " Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
